import os
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoModel, AutoTokenizer

from dataset import EmotionDataset
from model import EmotionClassifier


def train_model(model_name, label_level, epochs=3, batch_size=16):
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ - ëª¨ë¸ëª…: {model_name}, ë ˆë²¨: {label_level}, ì—í­: {epochs}, ë°°ì¹˜ì‚¬ì´ì¦ˆ: {batch_size}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤ ì„¤ì •: {'CUDA ì‚¬ìš©(GPU)' if torch.cuda.is_available() else 'CPU ì‚¬ìš©'}")

    # 1. í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ëª… ì§€ì •
    if model_name == "kcbert":
        pretrained = "beomi/kcbert-base"
    elif model_name == "koelectra":
        pretrained = "monologg/koelectra-base-discriminator"
    elif model_name == "klue":
        pretrained = "klue/roberta-base"
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ëª…ì…ë‹ˆë‹¤: {model_name}")

    # 2. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”©
    tokenizer = AutoTokenizer.from_pretrained(pretrained)
    base_model = AutoModel.from_pretrained(pretrained)
    model = EmotionClassifier(base_model, hidden_size=768, num_labels=None).to(device)
    print("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

    # 3. ë°ì´í„° ë¡œë”©
    csv_path = "D:/workspace/Project_test/data/sample1.csv"
    dataset = EmotionDataset(csv_path, levels=[label_level], model_name=model_name, tokenizer=tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ìƒ˜í”Œ")

    # 4. ë ˆì´ë¸” ìˆ˜ ì¶”ì¶œ ë° classifier ì—°ê²°
    num_labels = len(dataset.label_mappings[label_level])
    model.classifier = torch.nn.Linear(768, num_labels).to(device)
    print(f"ğŸ¯ í´ë˜ìŠ¤ ìˆ˜ ({label_level}): {num_labels}")

    # 5. ì˜µí‹°ë§ˆì´ì €
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # 6. í•™ìŠµ ë£¨í”„
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        print(f"\nğŸ“˜ Epoch {epoch+1}/{epochs}")
        for batch_idx, batch in enumerate(dataloader):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"][label_level].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = torch.nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            if (batch_idx + 1) % 10 == 0 or batch_idx == 0:
                print(f"  ğŸ”„ Step {batch_idx+1}/{len(dataloader)} | Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(dataloader)
        print(f"ğŸ“Š Epoch {epoch+1} ì™„ë£Œ | í‰ê·  Loss: {avg_loss:.4f}")

    # 7. ëª¨ë¸ ì €ì¥
    save_dir = f"./checkpoints/{model_name}_{label_level}"
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "model.pt")
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!\n")


# âœ… ì „ì²´ ëª¨ë¸-ë ˆë²¨ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
if __name__ == "__main__":
    models = ["kcbert", "koelectra", "klue"]
    levels = ["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜"]

    total_jobs = len(models) * len(levels)
    current_job = 1

    print(f"ğŸ§  ì „ì²´ í•™ìŠµ ì¡°í•© ìˆ˜: {total_jobs}ê°œ")

    for model_name in models:
        for label_level in levels:
            print(f"\n============================")
            print(f"â–¶ï¸ [{current_job}/{total_jobs}] ëª¨ë¸: {model_name.upper()} | ë ˆë²¨: {label_level}")
            print(f"============================")
            try:
                train_model(model_name, label_level, epochs=3, batch_size=16)
                print(f"âœ… í•™ìŠµ ì„±ê³µ â†’ ëª¨ë¸: {model_name.upper()}, ë ˆë²¨: {label_level}")
            except Exception as e:
                print(f"âŒ í•™ìŠµ ì‹¤íŒ¨ â†’ ëª¨ë¸: {model_name.upper()}, ë ˆë²¨: {label_level}")
                print(f"ì—ëŸ¬: {e}")
            current_job += 1

    print("\nğŸ‰ ì „ì²´ ëª¨ë¸/ë ˆë²¨ í•™ìŠµ ì™„ë£Œ!")
