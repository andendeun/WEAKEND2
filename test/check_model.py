import torch
from transformers import ElectraTokenizer, ElectraForSequenceClassification
import torch.nn.functional as F

# ì„¤ì •
MODEL_PATH = "koelectra_emotion.pt"
NUM_LABELS = 8

label_list = [
    "í–‰ë³µ/ê¸°ì¨/ê°ì‚¬",
    "ì‹ ë¢°/í¸ì•ˆ/ì¡´ê²½/ì•ˆì •",
    "ë¶„ë…¸/ì§œì¦/ë¶ˆí¸",
    "ë‹¹í™©/ì¶©ê²©/ë°°ì‹ ê°",
    "ê³µí¬/ë¶ˆì•ˆ",
    "ê³ ë…/ì™¸ë¡œì›€/ì†Œì™¸ê°/í—ˆíƒˆ",
    "ì£„ì±…ê°/ë¯¸ì•ˆí•¨",
    "ê±±ì •/ê³ ë¯¼/ê¸´ì¥"
]

# ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")
model = ElectraForSequenceClassification.from_pretrained(
    "monologg/koelectra-base-discriminator", num_labels=NUM_LABELS
)
model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
model.eval()

# ì˜ˆì¸¡ í•¨ìˆ˜ (í™•ë¥  í¬í•¨)
def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=1).squeeze()
        pred_idx = torch.argmax(probs).item()
        confidence = probs[pred_idx].item()
        
    return label_list[pred_idx], confidence, probs

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
texts = [
    "ì˜¤ëŠ˜ í•˜ë£¨ ë„ˆë¬´ ì™¸ë¡­ê³  í—ˆíƒˆí•´",
    "ì •ë§ ê°ì‚¬í•œ ë§ˆìŒì´ ë“¤ì–´",
    "ê±”ëŠ” ë‚˜í•œí…Œ ë°°ì‹ ê°ì„ ì¤¬ì–´",
    "ì†ì´ ë„ˆë¬´ ë¶ˆì•ˆí•˜ê³  ì´ˆì¡°í•´",
    "ì§œì¦ë‚˜ê³  í™”ê°€ ë‚˜"
]

# ì‹¤í–‰
print("\nğŸ“Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì‹œì‘:")
for text in texts:
    label, confidence, prob_tensor = predict_emotion(text)
    print(f"ğŸ—£ \"{text}\"")
    print(f"ğŸ”® ì˜ˆì¸¡: {label} ({confidence:.2%} í™•ë¥ )\n")
