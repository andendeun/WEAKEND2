import torch
from transformers import AutoModel, AutoTokenizer
from model import EmotionClassifier
from dataset import EmotionDataset

def load_model(model_name, label_level):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì§€ì •
    if model_name == "kcbert":
        pretrained = "beomi/kcbert-base"
    elif model_name == "koelectra":
        pretrained = "monologg/koelectra-base-discriminator"
    elif model_name == "klue":
        pretrained = "klue/roberta-base"
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.")

    # í† í¬ë‚˜ì´ì € ë° base model ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(pretrained)
    base_model = AutoModel.from_pretrained(pretrained)

    # ë ˆì´ë¸” ìˆ˜ ì¶”ì¶œì„ ìœ„í•´ ì„ì‹œ ë°ì´í„°ì…‹ ë¡œë“œ
    dummy_dataset = EmotionDataset(
        csv_path="D:/workspace/Project_test/data/sample1.csv",
        levels=[label_level],
        model_name=model_name,
        tokenizer=tokenizer
    )
    label_mapping = dummy_dataset.label_mappings[label_level]
    id2label = {v: k for k, v in label_mapping.items()}
    num_labels = len(label_mapping)

    # ëª¨ë¸ ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
    model = EmotionClassifier(base_model, hidden_size=768, num_labels=num_labels)
    checkpoint_path = f"./checkpoints/{model_name}_{label_level}/model.pt"
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.to(device)
    model.eval()

    return model, tokenizer, id2label, device

def predict(text, model, tokenizer, id2label, device):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        probs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])
    pred_id = torch.argmax(probs, dim=1).item()
    pred_label = id2label[pred_id]
    return pred_label, probs.squeeze().tolist()

if __name__ == "__main__":
    input_text = "ìš”ì¦˜ ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ì–´."  # ì˜ˆì‹œ ë¬¸ì¥
    models = ["kcbert", "koelectra", "klue"]
    levels = ["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜"]

    print(f"\nğŸ” ì…ë ¥ ë¬¸ì¥: {input_text}")
    print(f"ğŸ§ª ì´ ì¡°í•©: {len(models)} ëª¨ë¸ Ã— {len(levels)} ê°ì • ë ˆë²¨\n")

    for model_name in models:
        print(f"\nğŸ§  ëª¨ë¸: {model_name.upper()}")
        for label_level in levels:
            try:
                model, tokenizer, id2label, device = load_model(model_name, label_level)
                label, probs = predict(input_text, model, tokenizer, id2label, device)
                print(f"  - [{label_level}] ì˜ˆì¸¡ ê°ì •: {label} | Top Score: {max(probs):.4f}")
            except Exception as e:
                print(f"  - [{label_level}] âŒ ì‹¤íŒ¨ ({e})")
