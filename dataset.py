import os
import requests
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer

# âœ… Google Driveì—ì„œ sample1.xlsx ë‹¤ìš´ë¡œë“œ
XLSX_FILE_ID = "1_o7DRLRewzZfnRjKCexu-KNLfTFK8_gX"
xlsx_path = "data/sample1.xlsx"

def download_xlsx_from_drive(file_id, destination_path):
    if not os.path.exists(destination_path):
        print(f"ğŸ“¥ sample1.xlsx ë‹¤ìš´ë¡œë“œ ì¤‘: {destination_path}")
        os.makedirs(os.path.dirname(destination_path), exist_ok=True)
        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        response = requests.get(url, stream=True)
        with open(destination_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=32768):
                if chunk:
                    f.write(chunk)
        print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    else:
        print("âœ… sample1.xlsx ì´ë¯¸ ì¡´ì¬")

# âœ… ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
download_xlsx_from_drive(XLSX_FILE_ID, xlsx_path)

class EmotionDataset(Dataset):
    def __init__(self, file_path, tokenizer=None, levels=["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜"], max_length=128, model_name="kcbert"):
        """
        file_path: .xlsx íŒŒì¼ ê²½ë¡œ (ë¬¸ìì—´) ë˜ëŠ” .xlsx ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        tokenizer: ì‚¬ì „ ë¡œë”©ëœ í† í¬ë‚˜ì´ì € ê°ì²´. ì—†ìœ¼ë©´ model_nameì— ë”°ë¼ ë‚´ë¶€ ë¡œë“œ.
        levels: ìë™ ë§¤í•‘í•  ì—´ë“¤ (ì˜ˆ: ["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜"])
        max_length: í† í° ìµœëŒ€ ê¸¸ì´
        model_name: ëª¨ë¸ëª… (ì˜ˆ: "kcbert", "koelectra", "klue")
        """
        print("ğŸ“‚ XLSX ë¡œë”© ì‹œì‘...")
        if isinstance(file_path, list):
            dfs = [pd.read_excel(path, engine='openpyxl') for path in file_path]
            self.df = pd.concat(dfs, ignore_index=True)
        elif isinstance(file_path, str):
            self.df = pd.read_excel(file_path, engine='openpyxl')
        else:
            raise ValueError("file_pathëŠ” ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        self.df = self.df.dropna(subset=["ë¬¸ì¥"])
        self.levels = levels
        self.max_length = max_length

        print("ğŸ”§ ê°ì • ë ˆë²¨ ì¸ë±ìŠ¤ ë§¤í•‘ ì¤‘...")
        self.label_mappings = {}
        for level in levels:
            labels = self.df[level].fillna("NULL").tolist()
            unique_labels = sorted(set(labels))
            self.label_mappings[level] = {label: idx for idx, label in enumerate(unique_labels)}

        for level in levels:
            mapping = self.label_mappings[level]
            self.df[f"{level}_id"] = self.df[level].fillna("NULL").map(mapping)

        self.texts = self.df["ë¬¸ì¥"].tolist()

        if tokenizer is not None:
            self.tokenizer = tokenizer
            print("âœ… ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
        else:
            print(f"ğŸ” ëª¨ë¸ëª… ê¸°ë°˜ í† í¬ë‚˜ì´ì € ë¡œë”©: {model_name}")
            if model_name == "kcbert":
                pretrained = "beomi/kcbert-base"
            elif model_name == "koelectra":
                pretrained = "monologg/koelectra-base-discriminator"
            elif model_name == "klue":
                pretrained = "klue/roberta-base"
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ëª…ì…ë‹ˆë‹¤: {model_name}")

            self.tokenizer = AutoTokenizer.from_pretrained(pretrained)
            print(f"âœ… {model_name} í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        print("âœï¸ ë¬¸ì¥ í† í°í™” ì‹œì‘...")
        self.encodings = self.tokenizer(
            self.texts,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        print(f"âœ… í† í°í™” ì™„ë£Œ: ì´ {len(self.texts)}ê°œ ë¬¸ì¥\n")

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        labels = {
            level: torch.tensor(self.df.iloc[idx][f"{level}_id"])
            for level in self.levels
        }
        item["labels"] = labels
        return item

# âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
if __name__ == "__main__":
    print("ğŸ§ª EmotionDataset ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...\n")
    dataset = EmotionDataset(
        file_path=xlsx_path,
        levels=["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì†Œë¶„ë¥˜"],
        model_name="kcbert"
    )

    print("ğŸ“Š ë ˆì´ë¸” ë§¤í•‘ ê²°ê³¼:")
    for level, mapping in dataset.label_mappings.items():
        print(f"  {level}: {mapping}")

    print(f"\nğŸ“¦ ìƒ˜í”Œ ê°œìˆ˜: {len(dataset)}")
    print("ğŸ” ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸:")
    sample = dataset[0]
    print("input_ids[:10]:", sample["input_ids"][:10])
    print("attention_mask[:10]:", sample["attention_mask"][:10])
    print("labels:", {k: v.item() for k, v in sample["labels"].items()})
